# Copyright (c) 2023 Boston Dynamics AI Institute LLC. All rights reserved.

from typing import List, Optional, Tuple

import cv2
import numpy as np
import torch

try:
    from torchvision.ops import box_convert
except ImportError:
    print("Could not import box_convert. This is OK if you are only using the client.")


import sys
from typing import List, Optional
from .server_wrapper import ServerMixin, host_model, send_request, str_to_image

sys.path.insert(0, "yolov7/")
try:
    from models.experimental import attempt_load  # noqa: E402
    from utils.datasets import letterbox  # noqa: E402
    from utils.general import (  # noqa: E402
        check_img_size,
        non_max_suppression,
        scale_coords,
    )
    from utils.torch_utils import TracedModel  # noqa: E402
except Exception as e:
    print(f"Could not import yolov7. This is OK if you are only using the client. {e}")
sys.path.pop(0)

# Copyright (c) 2023 Boston Dynamics AI Institute LLC. All rights reserved.

COCO_CLASSES = [
    "person",
    "bicycle",
    "car",
    "motorcycle",
    "airplane",
    "bus",
    "train",
    "truck",
    "boat",
    "traffic light",
    "fire hydrant",
    "stop sign",
    "parking meter",
    "bench",
    "bird",
    "cat",
    "dog",
    "horse",
    "sheep",
    "cow",
    "elephant",
    "bear",
    "zebra",
    "giraffe",
    "backpack",
    "umbrella",
    "handbag",
    "tie",
    "suitcase",
    "frisbee",
    "skis",
    "snowboard",
    "sports ball",
    "kite",
    "baseball bat",
    "baseball glove",
    "skateboard",
    "surfboard",
    "tennis racket",
    "bottle",
    "wine glass",
    "cup",
    "fork",
    "knife",
    "spoon",
    "bowl",
    "banana",
    "apple",
    "sandwich",
    "orange",
    "broccoli",
    "carrot",
    "hot dog",
    "pizza",
    "donut",
    "cake",
    "chair",
    "couch",
    "potted plant",
    "bed",
    "dining table",
    "toilet",
    "tv",
    "laptop",
    "mouse",
    "remote",
    "keyboard",
    "cell phone",
    "microwave",
    "oven",
    "toaster",
    "sink",
    "refrigerator",
    "book",
    "clock",
    "vase",
    "scissors",
    "teddy bear",
    "hair drier",
    "toothbrush",
]

class ObjectDetections:
    """
    Provides a consistent format for object detections generated by both object
    detection and grounding models.
    """

    def __init__(
        self,
        boxes: torch.Tensor,
        logits: torch.Tensor,
        phrases: List[str],
        image_source: Optional[np.ndarray],
        fmt: str = "cxcywh",
    ):
        self.image_source = image_source
        if fmt != "xyxy":
            self.boxes = box_convert(boxes=boxes, in_fmt=fmt, out_fmt="xyxy")
        else:
            self.boxes = boxes
        self.logits = logits
        self.phrases = phrases
        self._annotated_frame: Optional[np.ndarray] = None

    @property
    def annotated_frame(self) -> Optional[np.ndarray]:
        if self._annotated_frame is None and self.image_source is not None:
            self._annotated_frame = annotate(
                image_source=self.image_source,
                boxes=self.boxes,
                logits=self.logits,
                phrases=self.phrases,
            )
        return self._annotated_frame

    @property
    def num_detections(self) -> int:
        """Returns the number of detections."""
        return len(self.phrases)

    def __repr__(self) -> str:
        """Print each detection's class, score, and box"""
        dets = [
            f"{phrase} ({logit:.2f}): {box.tolist()}"
            for box, logit, phrase in zip(self.boxes, self.logits, self.phrases)
        ]
        if len(dets) == 0:
            return "No detections"
        return "\n".join(dets)

    def filter_by_conf(self, conf_thresh: float) -> None:
        """Filters detections by confidence threshold in-place.

        Args:
            conf_thresh (float): Confidence threshold to filter detections.
        """
        keep: torch.Tensor = torch.ge(self.logits, conf_thresh)  # >=
        self._filter(keep)

    def filter_by_class(self, classes: List[str]) -> None:
        """Filters detections by class in-place.

        Args:
            classes (List[str]): List of classes to keep.
        """
        keep: torch.Tensor = torch.tensor([p in classes for p in self.phrases], dtype=torch.bool)
        self._filter(keep)

    def _filter(self, keep: torch.Tensor) -> None:
        """Filters detections in-place."""
        # Return early if no detections to filter
        if keep.all():
            return

        self.boxes = self.boxes[keep]
        self.logits = self.logits[keep]
        self.phrases = [p for i, p in enumerate(self.phrases) if keep[i]]
        self._annotated_frame = None

    def to_json(self) -> dict:
        """
        Converts the object detections to a JSON serializable format.

        Returns:
            dict: A dictionary containing the object detections.
        """
        return {
            "boxes": self.boxes.tolist(),
            "logits": self.logits.tolist(),
            "phrases": self.phrases,
        }

    @classmethod
    def from_json(
        cls,
        json_dict: dict,
        image_source: Optional[np.ndarray] = None,
    ) -> "ObjectDetections":
        """
        Converts the object detections from a JSON serializable format.

        Args:
            json_dict (dict): A dictionary containing the object detections.
            image_source (Optional[np.ndarray], optional): Optionally provide the
                original image source. Defaults to None.
        """
        return cls(
            image_source=image_source,
            boxes=torch.tensor(json_dict["boxes"]),
            logits=torch.tensor(json_dict["logits"]),
            phrases=json_dict["phrases"],
            fmt="xyxy",
        )


def annotate(
    image_source: np.ndarray,
    boxes: torch.Tensor,
    logits: torch.Tensor,
    phrases: List[str],
) -> np.ndarray:
    """
    Annotates an image with bounding boxes, class names, and scores.

    Args:
        image_source (np.ndarray): Input image in numpy array format.
        boxes (torch.Tensor): A tensor of shape (N, 4) containing the bounding boxes
            for each object in the image. The bounding boxes should be in the format
            (x1, y1, x2, y2).
        logits (torch.Tensor): A tensor of shape (N, C) containing the confidence
            scores for each object in the image.
        phrases (List[str]): A list of strings containing the class names for each
            object in the image.

    Returns:
        np.ndarray: The original image with the bounding boxes, class names, and
            scores labeled on it.
    """
    img = image_source.copy()

    # Draw bounding boxes, class names, and scores on image
    for box, prob, phrase in zip(boxes, logits, phrases):
        # Convert tensor to numpy array
        box = box.detach().cpu().numpy()
        prob = prob.detach().cpu().numpy()

        # If the box appears to be in normalized coordinates, de-normalize using the
        # image dimensions
        if box.max() <= 1:
            box = box * np.array([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])
            box = box.astype(int)

        # Draw bounding box
        point1 = (int(box[0]), int(box[1]))
        point2 = (int(box[2]), int(box[3]))
        img = draw_bounding_box(
            image=img,
            point1=point1,
            point2=point2,
            class_name=phrase,
            score=prob.max(),
        )

    return img


def draw_bounding_box(
    image: np.ndarray,
    point1: Tuple[int, int],
    point2: Tuple[int, int],
    class_name: str,
    score: float,
    color: Optional[Tuple[int, int, int]] = None,
) -> np.ndarray:
    """
    Draws a bounding box on an image and labels it with a class name and score.

    Args:
        image (np.ndarray): Input image in RGB numpy array format.
        point1 (Tuple[int, int]): The coordinates for the top left point of the bounding
            box.
        point2 (Tuple[int, int]): The coordinates for the bottom right point of the
            bounding box.
        class_name (str): A string representing the class name of the predicted object
            within the bounding box.
        score (float): A confidence score of the predicted object within the bounding
            box. This should be in the range of 0.0 to 1.0.
        color (Optional[Tuple[int, int, int]]): A tuple containing RGB values (in the
            range of 0-255) of the bounding box color. If None, the color will be
            randomly chosen.

    Returns:
        np.ndarray: The original image with the bounding box and corresponding class
            name and score labeled on it.
    """
    # Create a copy of the input image to draw on
    img = cv2.cvtColor(image.copy(), cv2.COLOR_RGB2BGR)

    if color is None:
        # Randomly choose a color from the rainbow colormap (so boxes aren't black)
        single_pixel = np.array([[np.random.randint(0, 256)]], dtype=np.uint8)
        single_pixel = cv2.applyColorMap(single_pixel, cv2.COLORMAP_RAINBOW)

        # reshape to a single dimensional array
        rand_color = single_pixel.reshape(3)
        bgr_color = [int(c) for c in rand_color]  # type: ignore
    else:
        # Convert RGB to BGR
        color = color[::-1]
        bgr_color = [int(c) for c in color]

    # Draw bounding box on image
    box_thickness = 2
    cv2.rectangle(img, point1, point2, bgr_color, thickness=box_thickness)

    # Draw class name and score on image
    text_label = f"{class_name}: {int(score * 100)}%"
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_thickness = 1
    text_size, _ = cv2.getTextSize(text_label, font, font_scale, font_thickness)
    text_x = point1[0]
    text_y = point2[1] + text_size[1]
    cv2.rectangle(
        img,
        (text_x, text_y - 2 * text_size[1]),
        (text_x + text_size[0], text_y - text_size[1]),
        bgr_color,
        -1,
    )
    cv2.putText(
        img,
        text_label,
        (text_x, text_y - text_size[1] - box_thickness),
        font,
        font_scale,
        (0, 0, 0),
        font_thickness,
    )

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    return img



class YOLOv7:
    _instance = None

    def __new__(cls, weights: str, image_size: int = 640, half_precision: bool = True):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, weights: str, image_size: int = 640, half_precision: bool = True):
        """Loads the model and saves it to a field."""
        if self._initialized:
            return
        self._initialized = True
        
        self.device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        # self.device = torch.device("cpu")
        self.half_precision = self.device.type != "cpu" and half_precision
        self.model = attempt_load(weights, map_location=self.device)  # load FP32 model
        stride = int(self.model.stride.max())  # model stride
        self.image_size = check_img_size(image_size, s=stride)  # check img_size
        self.model = TracedModel(self.model, self.device, self.image_size)
        if self.half_precision:
            self.model.half()  # to FP16

        # Warm-up
        if self.device.type != "cpu":
            dummy_img = torch.rand(1, 3, int(self.image_size * 0.7), self.image_size).to(self.device)
            if self.half_precision:
                dummy_img = dummy_img.half()
            for i in range(3):
                self.model(dummy_img)

    def predict(
        self,
        image: np.ndarray,
        conf_thres: float = 0.25,
        iou_thres: float = 0.45,
        classes: Optional[List[str]] = None,
        agnostic_nms: bool = False,
    ) -> ObjectDetections:
        """
        Outputs bounding box and class prediction data for the given image.

        Args:
            image (np.ndarray): An RGB image represented as a numpy array.
            conf_thres (float): Confidence threshold for filtering detections.
            iou_thres (float): IOU threshold for filtering detections.
            classes (list): List of classes to filter by.
            agnostic_nms (bool): Whether to use agnostic NMS.
        """
        orig_shape = image.shape
        # Preprocess image
        img = cv2.resize(
            image,
            (self.image_size, int(self.image_size * 0.7)),
            interpolation=cv2.INTER_AREA,
        )
        img = letterbox(img, new_shape=self.image_size)[0]
        img = img.transpose(2, 0, 1)  # BGR to RGB, to 3x416x416
        img = np.ascontiguousarray(img)

        img = torch.from_numpy(img).to(self.device)
        img = img.half() if self.half_precision else img.float()  # uint8 to fp16/32
        img /= 255.0  # 0 - 255 to 0.0 - 1.0
        if img.ndimension() == 3:
            img = img.unsqueeze(0)
        # Inference
        with torch.inference_mode():  # Calculating gradients causes a GPU memory leak
            pred = self.model(img)[0]
        # Apply NMS
        pred = non_max_suppression(
            pred,
            conf_thres,
            iou_thres,
            classes=classes,
            agnostic=agnostic_nms,
        )[0]
        # Rescale boxes from img_size to im0 size
        pred[:, :4] = scale_coords(img.shape[2:], pred[:, :4], orig_shape).round()
        pred[:, 0] /= orig_shape[1]
        pred[:, 1] /= orig_shape[0]
        pred[:, 2] /= orig_shape[1]
        pred[:, 3] /= orig_shape[0]
        boxes = pred[:, :4]
        logits = pred[:, 4]
        phrases = [COCO_CLASSES[int(i)] for i in pred[:, 5]]
        detections = ObjectDetections(boxes, logits, phrases, image_source=image, fmt="xyxy")
        return detections
    
class YOLOv7Client:
    def __init__(self, port: int = 12184):
        self.url = f"http://localhost:{port}/yolov7"

    def predict(self, image_numpy: np.ndarray) -> ObjectDetections:
        response = send_request(self.url, image=image_numpy)
        detections = ObjectDetections.from_json(response, image_source=image_numpy)

        return detections


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=12184)
    args = parser.parse_args()

    print("Loading model...")

    class YOLOv7Server(ServerMixin, YOLOv7):
        def process_payload(self, payload: dict) -> dict:
            image = str_to_image(payload["image"])
            return self.predict(image).to_json()

    yolov7 = YOLOv7Server("data/yolov7-e6e.pt")
    print("Model loaded!")
    print(f"Hosting on port {args.port}...")
    host_model(yolov7, name="yolov7", port=args.port)

    # Instantiate model
    # model = YOLOv7(weights="data/yolov7-e6e.pt")
    # img_path = "data/horses.jpg"
    # img = cv2.imread(img_path)
    # # Convert to RGB
    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    # # Predict
    # pred = model.predict(img)
    # print("Pred")
    # print(pred)


# global yolov7_model = YOLOv7(weights="data/yolov7-e6e.pt")